{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "chbGtI5FXp3t"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM984UDKtvVScW5jb66Onvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micolspitale93/COCOvoices/blob/master/Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing Feature Sets**\n",
        "We will create a csv file for each modality (visual, audio, and text) and save to be used for modeling."
      ],
      "metadata": {
        "id": "1EfjWaDPp1xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting drive, import libs, and constants declarations"
      ],
      "metadata": {
        "id": "KuKskFgyssRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "W6XK757pqGOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6fef4f-fea7-46fd-e564-54bae90a8235"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all imports go here\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "!pip install boruta\n",
        "from boruta import BorutaPy"
      ],
      "metadata": {
        "id": "RirhQFS4qCsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc72329-8530-4a38-b923-f231e3c31a41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->boruta) (1.2.0)\n",
            "Installing collected packages: boruta\n",
            "Successfully installed boruta-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path to data folder \n",
        "DATA_DIR = '/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/'\n",
        "MODALITY = [\"face\"]#[\"audio\", \"face\"]\n",
        "SUBDIR_MODALITY = [\"bsft_librosa/\", \"bsft_openface/collated/\"]\n",
        "\n",
        "# path to labels file \n",
        "LABELS_PATH = '/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/labels.xlsx'\n",
        "\n",
        "# path to results directory \n",
        "RESULTS_DIR = '/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/results/'\n",
        "\n",
        "# path to feature_sets directory \n",
        "FEATURE_DIR = '/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/feature_sets/'"
      ],
      "metadata": {
        "id": "loqom9hyqEPk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions definition"
      ],
      "metadata": {
        "id": "QEsfe3vqs2eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_outsideCV(input_df):\n",
        "  # drop columns with NaN values\n",
        "  df = input_df.dropna(axis='columns')\n",
        "  df = df.loc[:, (df != df.iloc[0]).any()] \n",
        "  return df"
      ],
      "metadata": {
        "id": "vwvLeSjjqLDi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_columns(df, column_to_ignore):\n",
        "  #for col in df.columns:\n",
        "  #  #if '_c' in col:\n",
        "  #    column_to_ignore.append(col)\n",
        "  colnames = []\n",
        "  colnames.append('filename')\n",
        "  colnames.append('label')\n",
        "  colnames.append('person_id')\n",
        "  colnames.append('week')\n",
        "  colnames.append('person_group')\n",
        "  colnames.append('week_group')\n",
        "  for col in df.columns:\n",
        "    if col not in column_to_ignore:\n",
        "      mean_fname = str(col) + '__mean'\n",
        "      median_fname = str(col) + '__median'\n",
        "      stddev_fname = str(col) + '__stddev'\n",
        "      autocorr_name = str(col) + '__autocorr'\n",
        "      colnames.append(mean_fname)\n",
        "      colnames.append(median_fname)\n",
        "      colnames.append(stddev_fname)\n",
        "      colnames.append(autocorr_name)\n",
        "  return colnames"
      ],
      "metadata": {
        "id": "2Hy5vchf0iLC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_fixed_length_vector(data, colnames, columns_to_ignore, data_dir):\n",
        "  # new dataframe that stores one fixed-length feature vector per video\n",
        "  # each vector contains the (1) mean, (2) standard deviation, (3) median, and (4) autocorrelation with lag 24\n",
        "  final_df = pd.DataFrame(columns=colnames)\n",
        "  first = True\n",
        "  print(len(data))\n",
        "  for filename in data:\n",
        "    if \"_S.csv\" in filename:\n",
        "      # read the data\n",
        "      print(data_dir + filename)\n",
        "      curr_df = pd.read_csv(data_dir + filename)\n",
        "      # new row values (the new row to be added)\n",
        "      new_row = []\n",
        "      # append the filename, label, person_id, and week number\n",
        "      new_row.append(filename)\n",
        "      new_row.append(label_dict[filename])\n",
        "      split_filename = filename.split('_')\n",
        "      person_id = split_filename[1]\n",
        "      week = split_filename[2]\n",
        "      new_row.append(person_id)\n",
        "      new_row.append(week)\n",
        "      if week == \"21Oct\" or week == \"18Nov\":\n",
        "        week_group = 1\n",
        "      elif week == \"27Oct\" or week == \"25Nov\":\n",
        "        week_group = 2\n",
        "      elif week == \"04Nov\" or week == \"02Dec\":\n",
        "        week_group = 3\n",
        "      elif week == \"11Nov\" or week == \"09Dec\":\n",
        "        week_group = 4\n",
        "      person_group = person_id.replace(\"S\",\"\")\n",
        "      new_row.append(person_group)\n",
        "      new_row.append(week_group)\n",
        "\n",
        "      # loop through each relevant column of this dataframe\n",
        "      for col in curr_df:\n",
        "        if col not in columns_to_ignore:\n",
        "          colvalues = curr_df[col].values\n",
        "          # compute standard time-series attributes\n",
        "          mean = np.mean(colvalues)\n",
        "          median = np.median(colvalues)\n",
        "          stddev = np.std(colvalues)\n",
        "\n",
        "          # compute autocorrelation with lag 24\n",
        "          autocorr = curr_df[col].autocorr(lag=24)  \n",
        "\n",
        "          # add to the new df row\n",
        "          new_row.append(mean)\n",
        "          new_row.append(median)\n",
        "          new_row.append(stddev)\n",
        "          new_row.append(autocorr)\n",
        "\n",
        "      new_row_series = pd.Series(new_row, index = final_df.columns)\n",
        "      final_df = final_df.append(new_row_series, ignore_index=True)\n",
        "      print(final_df.shape)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "eLuL441GqOjq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_label_dictonary(data_files, labels):\n",
        "  # this maps each file name to a label\n",
        "  label_dict = {}\n",
        "  for file in data_files:\n",
        "    splitfile = file.split('_')\n",
        "    participant_id = splitfile[1]\n",
        "    week = splitfile[2]\n",
        "    if week == \"21Oct\" or week == \"18Nov\":\n",
        "      week = 1\n",
        "    elif week == \"27Oct\" or week == \"25Nov\":\n",
        "      week = 2\n",
        "    elif week == \"04Nov\" or week == \"02Dec\":\n",
        "      week = 3\n",
        "    elif week == \"11Nov\" or week == \"09Dec\":\n",
        "      week = 4\n",
        "    role = splitfile[3]\n",
        "    label = labels[labels['participantID_week']==participant_id + \"_\"+str(week)][\"label\"].values[0] #TODO: to check according to the labels file\n",
        "    label_dict[file] = label\n",
        "  return label_dict"
      ],
      "metadata": {
        "id": "J3mek7qjqM-L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running code for preparing features"
      ],
      "metadata": {
        "id": "4890J9iSs-FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the labels\n",
        "labels = pd.read_excel(LABELS_PATH, sheet_name='labels')\n",
        "#print(labels.head())\n",
        "mode_label_dict = {}\n",
        "mode_colnames = {}\n",
        "for mode in MODALITY:\n",
        "  if mode == \"audio\":\n",
        "    columns_to_ignore  = []\n",
        "    subdir = SUBDIR_MODALITY[0]\n",
        "  elif mode == \"face\":\n",
        "    columns_to_ignore = ['frame', 'face_id', 'timestamp', 'confidence', 'success']\n",
        "    subdir = SUBDIR_MODALITY[1]\n",
        "  data_files = os.listdir(DATA_DIR + subdir)\n",
        "  sample_df = pd.read_csv(DATA_DIR + subdir+  data_files[0])\n",
        "  label_dict = make_label_dictonary(data_files, labels)\n",
        "  col_names = define_columns(sample_df, columns_to_ignore)\n",
        "  print(col_names)\n",
        "  prepared_df = prepare_fixed_length_vector(data_files, col_names, columns_to_ignore, DATA_DIR + subdir)\n",
        "  prepared_df.to_csv(FEATURE_DIR + mode +'_fixed_length_vectors.csv')\n",
        "  mode_label_dict[mode] = label_dict\n",
        "  mode_colnames[mode] = col_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v0U6tpj9RNO",
        "outputId": "071a730f-3f7a-4362-fedd-247ae10b0cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['filename', 'label', 'person_id', 'week', 'person_group', 'week_group', 'mel_1__mean', 'mel_1__median', 'mel_1__stddev', 'mel_1__autocorr', 'mel_2__mean', 'mel_2__median', 'mel_2__stddev', 'mel_2__autocorr', 'mel_3__mean', 'mel_3__median', 'mel_3__stddev', 'mel_3__autocorr', 'mel_4__mean', 'mel_4__median', 'mel_4__stddev', 'mel_4__autocorr', 'mel_5__mean', 'mel_5__median', 'mel_5__stddev', 'mel_5__autocorr', 'mel_6__mean', 'mel_6__median', 'mel_6__stddev', 'mel_6__autocorr', 'mel_7__mean', 'mel_7__median', 'mel_7__stddev', 'mel_7__autocorr', 'mel_8__mean', 'mel_8__median', 'mel_8__stddev', 'mel_8__autocorr', 'mel_9__mean', 'mel_9__median', 'mel_9__stddev', 'mel_9__autocorr', 'mel_10__mean', 'mel_10__median', 'mel_10__stddev', 'mel_10__autocorr', 'mel_11__mean', 'mel_11__median', 'mel_11__stddev', 'mel_11__autocorr', 'mel_12__mean', 'mel_12__median', 'mel_12__stddev', 'mel_12__autocorr', 'mel_13__mean', 'mel_13__median', 'mel_13__stddev', 'mel_13__autocorr', 'mel_14__mean', 'mel_14__median', 'mel_14__stddev', 'mel_14__autocorr', 'mel_15__mean', 'mel_15__median', 'mel_15__stddev', 'mel_15__autocorr', 'mel_16__mean', 'mel_16__median', 'mel_16__stddev', 'mel_16__autocorr', 'mel_17__mean', 'mel_17__median', 'mel_17__stddev', 'mel_17__autocorr', 'mel_18__mean', 'mel_18__median', 'mel_18__stddev', 'mel_18__autocorr', 'mel_19__mean', 'mel_19__median', 'mel_19__stddev', 'mel_19__autocorr', 'mel_20__mean', 'mel_20__median', 'mel_20__stddev', 'mel_20__autocorr', 'mel_21__mean', 'mel_21__median', 'mel_21__stddev', 'mel_21__autocorr', 'mel_22__mean', 'mel_22__median', 'mel_22__stddev', 'mel_22__autocorr', 'mel_23__mean', 'mel_23__median', 'mel_23__stddev', 'mel_23__autocorr', 'mel_24__mean', 'mel_24__median', 'mel_24__stddev', 'mel_24__autocorr', 'mel_25__mean', 'mel_25__median', 'mel_25__stddev', 'mel_25__autocorr', 'mel_26__mean', 'mel_26__median', 'mel_26__stddev', 'mel_26__autocorr', 'mel_27__mean', 'mel_27__median', 'mel_27__stddev', 'mel_27__autocorr', 'mel_28__mean', 'mel_28__median', 'mel_28__stddev', 'mel_28__autocorr', 'mel_29__mean', 'mel_29__median', 'mel_29__stddev', 'mel_29__autocorr', 'mel_30__mean', 'mel_30__median', 'mel_30__stddev', 'mel_30__autocorr', 'mel_31__mean', 'mel_31__median', 'mel_31__stddev', 'mel_31__autocorr', 'mel_32__mean', 'mel_32__median', 'mel_32__stddev', 'mel_32__autocorr', 'mel_33__mean', 'mel_33__median', 'mel_33__stddev', 'mel_33__autocorr', 'mel_34__mean', 'mel_34__median', 'mel_34__stddev', 'mel_34__autocorr', 'mel_35__mean', 'mel_35__median', 'mel_35__stddev', 'mel_35__autocorr', 'mel_36__mean', 'mel_36__median', 'mel_36__stddev', 'mel_36__autocorr', 'mel_37__mean', 'mel_37__median', 'mel_37__stddev', 'mel_37__autocorr', 'mel_38__mean', 'mel_38__median', 'mel_38__stddev', 'mel_38__autocorr', 'mel_39__mean', 'mel_39__median', 'mel_39__stddev', 'mel_39__autocorr', 'mel_40__mean', 'mel_40__median', 'mel_40__stddev', 'mel_40__autocorr', 'mel_41__mean', 'mel_41__median', 'mel_41__stddev', 'mel_41__autocorr', 'mel_42__mean', 'mel_42__median', 'mel_42__stddev', 'mel_42__autocorr', 'mel_43__mean', 'mel_43__median', 'mel_43__stddev', 'mel_43__autocorr', 'mel_44__mean', 'mel_44__median', 'mel_44__stddev', 'mel_44__autocorr', 'mel_45__mean', 'mel_45__median', 'mel_45__stddev', 'mel_45__autocorr', 'mel_46__mean', 'mel_46__median', 'mel_46__stddev', 'mel_46__autocorr', 'mel_47__mean', 'mel_47__median', 'mel_47__stddev', 'mel_47__autocorr', 'mel_48__mean', 'mel_48__median', 'mel_48__stddev', 'mel_48__autocorr', 'mel_49__mean', 'mel_49__median', 'mel_49__stddev', 'mel_49__autocorr', 'mel_50__mean', 'mel_50__median', 'mel_50__stddev', 'mel_50__autocorr', 'mel_51__mean', 'mel_51__median', 'mel_51__stddev', 'mel_51__autocorr', 'mel_52__mean', 'mel_52__median', 'mel_52__stddev', 'mel_52__autocorr', 'mel_53__mean', 'mel_53__median', 'mel_53__stddev', 'mel_53__autocorr', 'mel_54__mean', 'mel_54__median', 'mel_54__stddev', 'mel_54__autocorr', 'mel_55__mean', 'mel_55__median', 'mel_55__stddev', 'mel_55__autocorr', 'mel_56__mean', 'mel_56__median', 'mel_56__stddev', 'mel_56__autocorr', 'mel_57__mean', 'mel_57__median', 'mel_57__stddev', 'mel_57__autocorr', 'mel_58__mean', 'mel_58__median', 'mel_58__stddev', 'mel_58__autocorr', 'mel_59__mean', 'mel_59__median', 'mel_59__stddev', 'mel_59__autocorr', 'mel_60__mean', 'mel_60__median', 'mel_60__stddev', 'mel_60__autocorr', 'mel_61__mean', 'mel_61__median', 'mel_61__stddev', 'mel_61__autocorr', 'mel_62__mean', 'mel_62__median', 'mel_62__stddev', 'mel_62__autocorr', 'mel_63__mean', 'mel_63__median', 'mel_63__stddev', 'mel_63__autocorr', 'mel_64__mean', 'mel_64__median', 'mel_64__stddev', 'mel_64__autocorr', 'mel_65__mean', 'mel_65__median', 'mel_65__stddev', 'mel_65__autocorr', 'mel_66__mean', 'mel_66__median', 'mel_66__stddev', 'mel_66__autocorr', 'mel_67__mean', 'mel_67__median', 'mel_67__stddev', 'mel_67__autocorr', 'mel_68__mean', 'mel_68__median', 'mel_68__stddev', 'mel_68__autocorr', 'mel_69__mean', 'mel_69__median', 'mel_69__stddev', 'mel_69__autocorr', 'mel_70__mean', 'mel_70__median', 'mel_70__stddev', 'mel_70__autocorr', 'mel_71__mean', 'mel_71__median', 'mel_71__stddev', 'mel_71__autocorr', 'mel_72__mean', 'mel_72__median', 'mel_72__stddev', 'mel_72__autocorr', 'mel_73__mean', 'mel_73__median', 'mel_73__stddev', 'mel_73__autocorr', 'mel_74__mean', 'mel_74__median', 'mel_74__stddev', 'mel_74__autocorr', 'mel_75__mean', 'mel_75__median', 'mel_75__stddev', 'mel_75__autocorr', 'mel_76__mean', 'mel_76__median', 'mel_76__stddev', 'mel_76__autocorr', 'mel_77__mean', 'mel_77__median', 'mel_77__stddev', 'mel_77__autocorr', 'mel_78__mean', 'mel_78__median', 'mel_78__stddev', 'mel_78__autocorr', 'mel_79__mean', 'mel_79__median', 'mel_79__stddev', 'mel_79__autocorr', 'mel_80__mean', 'mel_80__median', 'mel_80__stddev', 'mel_80__autocorr', 'mel_81__mean', 'mel_81__median', 'mel_81__stddev', 'mel_81__autocorr', 'mel_82__mean', 'mel_82__median', 'mel_82__stddev', 'mel_82__autocorr', 'mel_83__mean', 'mel_83__median', 'mel_83__stddev', 'mel_83__autocorr', 'mel_84__mean', 'mel_84__median', 'mel_84__stddev', 'mel_84__autocorr', 'mel_85__mean', 'mel_85__median', 'mel_85__stddev', 'mel_85__autocorr', 'mel_86__mean', 'mel_86__median', 'mel_86__stddev', 'mel_86__autocorr', 'mel_87__mean', 'mel_87__median', 'mel_87__stddev', 'mel_87__autocorr', 'mel_88__mean', 'mel_88__median', 'mel_88__stddev', 'mel_88__autocorr', 'mel_89__mean', 'mel_89__median', 'mel_89__stddev', 'mel_89__autocorr', 'mel_90__mean', 'mel_90__median', 'mel_90__stddev', 'mel_90__autocorr', 'mel_91__mean', 'mel_91__median', 'mel_91__stddev', 'mel_91__autocorr', 'mel_92__mean', 'mel_92__median', 'mel_92__stddev', 'mel_92__autocorr', 'mel_93__mean', 'mel_93__median', 'mel_93__stddev', 'mel_93__autocorr', 'mel_94__mean', 'mel_94__median', 'mel_94__stddev', 'mel_94__autocorr', 'mel_95__mean', 'mel_95__median', 'mel_95__stddev', 'mel_95__autocorr', 'mel_96__mean', 'mel_96__median', 'mel_96__stddev', 'mel_96__autocorr', 'mel_97__mean', 'mel_97__median', 'mel_97__stddev', 'mel_97__autocorr', 'mel_98__mean', 'mel_98__median', 'mel_98__stddev', 'mel_98__autocorr', 'mel_99__mean', 'mel_99__median', 'mel_99__stddev', 'mel_99__autocorr', 'mel_100__mean', 'mel_100__median', 'mel_100__stddev', 'mel_100__autocorr', 'mel_101__mean', 'mel_101__median', 'mel_101__stddev', 'mel_101__autocorr', 'mel_102__mean', 'mel_102__median', 'mel_102__stddev', 'mel_102__autocorr', 'mel_103__mean', 'mel_103__median', 'mel_103__stddev', 'mel_103__autocorr', 'mel_104__mean', 'mel_104__median', 'mel_104__stddev', 'mel_104__autocorr', 'mel_105__mean', 'mel_105__median', 'mel_105__stddev', 'mel_105__autocorr', 'mel_106__mean', 'mel_106__median', 'mel_106__stddev', 'mel_106__autocorr', 'mel_107__mean', 'mel_107__median', 'mel_107__stddev', 'mel_107__autocorr', 'mel_108__mean', 'mel_108__median', 'mel_108__stddev', 'mel_108__autocorr', 'mel_109__mean', 'mel_109__median', 'mel_109__stddev', 'mel_109__autocorr', 'mel_110__mean', 'mel_110__median', 'mel_110__stddev', 'mel_110__autocorr', 'mel_111__mean', 'mel_111__median', 'mel_111__stddev', 'mel_111__autocorr', 'mel_112__mean', 'mel_112__median', 'mel_112__stddev', 'mel_112__autocorr', 'mel_113__mean', 'mel_113__median', 'mel_113__stddev', 'mel_113__autocorr', 'mel_114__mean', 'mel_114__median', 'mel_114__stddev', 'mel_114__autocorr', 'mel_115__mean', 'mel_115__median', 'mel_115__stddev', 'mel_115__autocorr', 'mel_116__mean', 'mel_116__median', 'mel_116__stddev', 'mel_116__autocorr', 'mel_117__mean', 'mel_117__median', 'mel_117__stddev', 'mel_117__autocorr', 'mel_118__mean', 'mel_118__median', 'mel_118__stddev', 'mel_118__autocorr', 'mel_119__mean', 'mel_119__median', 'mel_119__stddev', 'mel_119__autocorr', 'mel_120__mean', 'mel_120__median', 'mel_120__stddev', 'mel_120__autocorr', 'mel_121__mean', 'mel_121__median', 'mel_121__stddev', 'mel_121__autocorr', 'mel_122__mean', 'mel_122__median', 'mel_122__stddev', 'mel_122__autocorr', 'mel_123__mean', 'mel_123__median', 'mel_123__stddev', 'mel_123__autocorr', 'mel_124__mean', 'mel_124__median', 'mel_124__stddev', 'mel_124__autocorr', 'mel_125__mean', 'mel_125__median', 'mel_125__stddev', 'mel_125__autocorr', 'mel_126__mean', 'mel_126__median', 'mel_126__stddev', 'mel_126__autocorr', 'mel_127__mean', 'mel_127__median', 'mel_127__stddev', 'mel_127__autocorr', 'mel_128__mean', 'mel_128__median', 'mel_128__stddev', 'mel_128__autocorr', 'mfcc_1__mean', 'mfcc_1__median', 'mfcc_1__stddev', 'mfcc_1__autocorr', 'mfcc_2__mean', 'mfcc_2__median', 'mfcc_2__stddev', 'mfcc_2__autocorr', 'mfcc_3__mean', 'mfcc_3__median', 'mfcc_3__stddev', 'mfcc_3__autocorr', 'mfcc_4__mean', 'mfcc_4__median', 'mfcc_4__stddev', 'mfcc_4__autocorr', 'mfcc_5__mean', 'mfcc_5__median', 'mfcc_5__stddev', 'mfcc_5__autocorr', 'mfcc_6__mean', 'mfcc_6__median', 'mfcc_6__stddev', 'mfcc_6__autocorr', 'mfcc_7__mean', 'mfcc_7__median', 'mfcc_7__stddev', 'mfcc_7__autocorr', 'mfcc_8__mean', 'mfcc_8__median', 'mfcc_8__stddev', 'mfcc_8__autocorr', 'mfcc_9__mean', 'mfcc_9__median', 'mfcc_9__stddev', 'mfcc_9__autocorr', 'mfcc_10__mean', 'mfcc_10__median', 'mfcc_10__stddev', 'mfcc_10__autocorr', 'mfcc_11__mean', 'mfcc_11__median', 'mfcc_11__stddev', 'mfcc_11__autocorr', 'mfcc_12__mean', 'mfcc_12__median', 'mfcc_12__stddev', 'mfcc_12__autocorr', 'mfcc_13__mean', 'mfcc_13__median', 'mfcc_13__stddev', 'mfcc_13__autocorr', 'mfcc_14__mean', 'mfcc_14__median', 'mfcc_14__stddev', 'mfcc_14__autocorr', 'mfcc_15__mean', 'mfcc_15__median', 'mfcc_15__stddev', 'mfcc_15__autocorr', 'mfcc_16__mean', 'mfcc_16__median', 'mfcc_16__stddev', 'mfcc_16__autocorr', 'mfcc_17__mean', 'mfcc_17__median', 'mfcc_17__stddev', 'mfcc_17__autocorr', 'mfcc_18__mean', 'mfcc_18__median', 'mfcc_18__stddev', 'mfcc_18__autocorr', 'mfcc_19__mean', 'mfcc_19__median', 'mfcc_19__stddev', 'mfcc_19__autocorr', 'mfcc_20__mean', 'mfcc_20__median', 'mfcc_20__stddev', 'mfcc_20__autocorr', 'delta_mfcc_1__mean', 'delta_mfcc_1__median', 'delta_mfcc_1__stddev', 'delta_mfcc_1__autocorr', 'delta_mfcc_2__mean', 'delta_mfcc_2__median', 'delta_mfcc_2__stddev', 'delta_mfcc_2__autocorr', 'delta_mfcc_3__mean', 'delta_mfcc_3__median', 'delta_mfcc_3__stddev', 'delta_mfcc_3__autocorr', 'delta_mfcc_4__mean', 'delta_mfcc_4__median', 'delta_mfcc_4__stddev', 'delta_mfcc_4__autocorr', 'delta_mfcc_5__mean', 'delta_mfcc_5__median', 'delta_mfcc_5__stddev', 'delta_mfcc_5__autocorr', 'delta_mfcc_6__mean', 'delta_mfcc_6__median', 'delta_mfcc_6__stddev', 'delta_mfcc_6__autocorr', 'delta_mfcc_7__mean', 'delta_mfcc_7__median', 'delta_mfcc_7__stddev', 'delta_mfcc_7__autocorr', 'delta_mfcc_8__mean', 'delta_mfcc_8__median', 'delta_mfcc_8__stddev', 'delta_mfcc_8__autocorr', 'delta_mfcc_9__mean', 'delta_mfcc_9__median', 'delta_mfcc_9__stddev', 'delta_mfcc_9__autocorr', 'delta_mfcc_10__mean', 'delta_mfcc_10__median', 'delta_mfcc_10__stddev', 'delta_mfcc_10__autocorr', 'delta_mfcc_11__mean', 'delta_mfcc_11__median', 'delta_mfcc_11__stddev', 'delta_mfcc_11__autocorr', 'delta_mfcc_12__mean', 'delta_mfcc_12__median', 'delta_mfcc_12__stddev', 'delta_mfcc_12__autocorr', 'delta_mfcc_13__mean', 'delta_mfcc_13__median', 'delta_mfcc_13__stddev', 'delta_mfcc_13__autocorr', 'delta_mfcc_14__mean', 'delta_mfcc_14__median', 'delta_mfcc_14__stddev', 'delta_mfcc_14__autocorr', 'delta_mfcc_15__mean', 'delta_mfcc_15__median', 'delta_mfcc_15__stddev', 'delta_mfcc_15__autocorr', 'delta_mfcc_16__mean', 'delta_mfcc_16__median', 'delta_mfcc_16__stddev', 'delta_mfcc_16__autocorr', 'delta_mfcc_17__mean', 'delta_mfcc_17__median', 'delta_mfcc_17__stddev', 'delta_mfcc_17__autocorr', 'delta_mfcc_18__mean', 'delta_mfcc_18__median', 'delta_mfcc_18__stddev', 'delta_mfcc_18__autocorr', 'delta_mfcc_19__mean', 'delta_mfcc_19__median', 'delta_mfcc_19__stddev', 'delta_mfcc_19__autocorr', 'delta_mfcc_20__mean', 'delta_mfcc_20__median', 'delta_mfcc_20__stddev', 'delta_mfcc_20__autocorr', 'spcentr_1__mean', 'spcentr_1__median', 'spcentr_1__stddev', 'spcentr_1__autocorr', 'rms_1__mean', 'rms_1__median', 'rms_1__stddev', 'rms_1__autocorr']\n",
            "41\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S4_27Oct_S.csv\n",
            "(1, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S5_04Nov_S.csv\n",
            "(2, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S5_11Nov_S.csv\n",
            "(3, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S4_21Oct_S.csv\n",
            "(4, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S4_04Nov_S.csv\n",
            "(5, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S4_11Nov_S.csv\n",
            "(6, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S3_27Oct_S.csv\n",
            "(7, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S3_11Nov_S.csv\n",
            "(8, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S3_21Oct_S.csv\n",
            "(9, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S2_21Oct_S.csv\n",
            "(10, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S2_27Oct_S.csv\n",
            "(11, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S2_11Nov_S.csv\n",
            "(12, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S1_27Oct_S.csv\n",
            "(13, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S2_04Nov_S.csv\n",
            "(14, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S1_11Nov_S.csv\n",
            "(15, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S1_04Nov_S.csv\n",
            "(16, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S10_18Nov_S.csv\n",
            "(17, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S11_25Nov_S.csv\n",
            "(18, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S11_18Nov_S.csv\n",
            "(19, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S11_02Dec_S.csv\n",
            "(20, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S11_09Dec_S.csv\n",
            "(21, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S10_25Nov_S.csv\n",
            "(22, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S10_02Dec_S.csv\n",
            "(23, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S10_09Dec_S.csv\n",
            "(24, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S9_25Nov_S.csv\n",
            "(25, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S9_09Dec_S.csv\n",
            "(26, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S9_18Nov_S.csv\n",
            "(27, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S9_02Dec_S.csv\n",
            "(28, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S8_18Nov_S.csv\n",
            "(29, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S8_25Nov_S.csv\n",
            "(30, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S8_09Dec_S.csv\n",
            "(31, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S7_25Nov_S.csv\n",
            "(32, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S8_02Dec_S.csv\n",
            "(33, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S7_18Nov_S.csv\n",
            "(34, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S7_09Dec_S.csv\n",
            "(35, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S6_27Oct_S.csv\n",
            "(36, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S7_02Dec_S.csv\n",
            "(37, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S6_21Oct_S.csv\n",
            "(38, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S6_04Nov_S.csv\n",
            "(39, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S6_11Nov_S.csv\n",
            "(40, 686)\n",
            "/content/drive/Shareddrives/Affective Intelligence & Robotics Lab (AFAR)/Projects/Jiaee Micol Project/BSFT_dataset_features_extracted/bsft_librosa/SK_S5_27Oct_S.csv\n",
            "(41, 686)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unimodal modeling**\n",
        "In this section we will predict XX via monomodal approaches using BSFT dataset.\n",
        "The ML models selected are:\n",
        "- 'logistic_regression',\n",
        "-        'rbf_svm',\n",
        "-          'decision_tree',\n",
        "-          'linear_svm',\n",
        "-          'adaboost',\n",
        "-          'xgboost',\n",
        "-          'bagging',\n",
        "-          'rforest'\n",
        "\n",
        "The basic deep learning models are:\n",
        "- 'LSTM'\n",
        "- 'TCN'\n",
        "\n",
        "We will predict XX via audio, face, or text. "
      ],
      "metadata": {
        "id": "chbGtI5FXp3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libs and dataframe definition"
      ],
      "metadata": {
        "id": "LS-F7V1-tG7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ltiOw-AqXopF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930be2b6-266f-4d1c-ec07-903e65bb6e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.2-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.41)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.0.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 65.8 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 62.9 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=3695bd33a71f795f718b14f8fe6fba35dd0285d1139bbce97fc71f337f05587b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.2 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ],
      "source": [
        "from xgboost import plot_importance\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from xgboost import XGBClassifier\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import warnings\n",
        "import itertools\n",
        "optuna.logging.set_verbosity(optuna.logging.FATAL)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import LeaveOneOut, LeaveOneGroupOut"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the label proportions\n",
        "DF_fixed_length = {}\n",
        "for mode in MODALITY:\n",
        "  DF_fixed_length[mode] = pd.read_csv(FEATURE_DIR + mode+'_fixed_length_vectors.csv')\n",
        "  label_proportion = DF_fixed_length[mode]['label'].sum()/len(DF_fixed_length[mode])\n",
        "  print('BASELINE (classifier that always predicts \"Positive Affect level\"): ', label_proportion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UajYk8CCp-gG",
        "outputId": "97d53321-d467-4425-fc91-3bc8f4bead56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE (classifier that always predicts \"Positive Affect level\"):  0.5853658536585366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions definitions"
      ],
      "metadata": {
        "id": "vTGrQCXutM6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial, clf_name, X, y):\n",
        "    if clf_name == 'logistic_regression':\n",
        "        C = trial.suggest_loguniform('C', 1e-2, 1e+2)\n",
        "        clf_model = LogisticRegression(C=C, max_iter=1e+5, solver='liblinear' , random_state=RANDOM_STATE)\n",
        "   \n",
        "    elif clf_name == 'linear_svm':\n",
        "        C = trial.suggest_loguniform('C', 1e-2, 1e+2)\n",
        "        degree = trial.suggest_int('degree',1, 50)\n",
        "        gamma = trial.suggest_loguniform('gamma',0.001,10000)\n",
        "        clf_model = SVC(C=C, kernel='linear', degree=degree,gamma=gamma, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'rbf_svm': \n",
        "        C = trial.suggest_loguniform('C', 1e-2, 1e+2)\n",
        "        degree = trial.suggest_int('degree',1, 50)\n",
        "        gamma = trial.suggest_loguniform('gamma',1e-2,1e+2)\n",
        "        clf_model = SVC(C=C, kernel='rbf', degree=degree,gamma=gamma, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'decision_tree':\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 20)\n",
        "        clf_model = DecisionTreeClassifier(max_depth=max_depth, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'rforest':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 2, 20)\n",
        "        max_depth = int(trial.suggest_float('max_depth', 1, 32, log=True))\n",
        "        clf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'adaboost':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 20, 100)\n",
        "        clf_model = AdaBoostClassifier(n_estimators=n_estimators, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'xgboost':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 20, 100)\n",
        "        clf_model = xgb.XGBClassifier(n_estimators=n_estimators, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'bagging':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 20, 100)\n",
        "        clf_model = BaggingClassifier(n_estimators=n_estimators, random_state=RANDOM_STATE)\n",
        "\n",
        "    metrics = {'acc' : 'accuracy',\n",
        "               'auc' : 'roc_auc',\n",
        "               'f1' : 'f1'}\n",
        "    clf = make_pipeline(MinMaxScaler(), clf_model)\n",
        "    score = cross_validate(clf, X, y, scoring=metrics)\n",
        "    return score['test_acc'].mean()"
      ],
      "metadata": {
        "id": "CoY1pu7_Zsrs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define models\n",
        "MODELS = ['logistic_regression',\n",
        "          'rbf_svm',\n",
        "          'decision_tree',\n",
        "          'linear_svm',\n",
        "          'adaboost',\n",
        "          'xgboost',\n",
        "          'bagging',\n",
        "          'rforest']\n",
        "# define modeling constants\n",
        "#GROUP = \"WEEK\" # WEEK or PERSON\n",
        "NON_MODELING_COLS = ['week_group', 'person_group', 'week', 'label', 'person_id', 'filename'] # columns not used in X features\n",
        "RANDOM_STATE = 0 # random state for all models and cross-validation splits\n",
        "NUM_SPLITS = 5 # number of cross-validation splits for model evaluation\n",
        "NUM_REPEATS = 10 # number of times we will repeat the cross-validation (with different splits)"
      ],
      "metadata": {
        "id": "5jAGIQI2ZuTj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_modeling_experiment_logo(df, num_splits, num_repeats, NT, verbose, group):\n",
        "  model_predictions = {} # predictions made by each model\n",
        "  model_ground_truths = {} # should all be the same, but might as well store these\n",
        "  model_features_ranked = {} # features ranked during features-selection\n",
        "  model_feature_importance = {}\n",
        "  model_features_selected = {} # features selected during feature-selection\n",
        "  model_params = {} # params of trained models\n",
        "  model_cm = {} # confusion matrix of trained models\n",
        "  model_probas = {}\n",
        "\n",
        "  # store raw results across all folds\n",
        "  all_model_results = pd.DataFrame(data=None, columns=['model', 'cv_fold', 'accuracy', 'auc', 'precision', 'recall', 'f1', 'feature_importance'])\n",
        "  all_values = pd.DataFrame(data=None, columns=['model', 'cv_fold', 'ground_truths', 'probas', 'preds'])\n",
        "\n",
        "  df_without_NaN = pre_process_outsideCV(df)\n",
        "  # loop through models\n",
        "  for model in MODELS:\n",
        "    # print model\n",
        "    print(model)\n",
        "    x_old = df_without_NaN\n",
        "    # get X and y\n",
        "    y = df_without_NaN['label']\n",
        "    X = df_without_NaN.drop(NON_MODELING_COLS, axis=1)\n",
        "    groups_week = x_old[\"week_group\"]\n",
        "    groups_participants = x_old[\"person_group\"]\n",
        "    ids = x_old[\"filename\"]\n",
        "    participants = x_old[\"person_id\"]\n",
        "    weeks = x_old[\"week\"]\n",
        "    if group == \"WEEK\":\n",
        "      groups = groups_week\n",
        "    elif group == \"PERSON\":\n",
        "      groups = groups_participants\n",
        "    # enter cross-validation loop\n",
        "    #rskf = RepeatedStratifiedKFold(n_splits=num_splits, n_repeats=num_repeats, random_state = RANDOM_STATE)\n",
        "    logo = LeaveOneGroupOut()\n",
        "\n",
        "\n",
        "    # initialize fold numbers and predictions/ground truth lists\n",
        "    fold_num = 0\n",
        "    predictions = []\n",
        "    probs = []\n",
        "    ground_truths = [] #Y_test\n",
        "    features_ranked = []\n",
        "    features_importance = []\n",
        "    \n",
        "\n",
        "\n",
        "    #for train_index, test_index in rskf.split(X, y):\n",
        "    for train_index, test_index in logo.split(X, y, groups=groups):\n",
        "      # get train and test indices\n",
        "      #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "      id_test = ids.loc[test_index]\n",
        "      participant_test = participants.loc[test_index]\n",
        "      picture_test = weeks.loc[test_index]\n",
        "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "      y_train, y_test = y.values[train_index], y.values[test_index]\n",
        "      # get the scaled version of the train and test set\n",
        "      mm = MinMaxScaler()\n",
        "      X_train_scaled = pd.DataFrame(mm.fit_transform(X_train), columns= X_train.columns)\n",
        "      X_test_scaled = pd.DataFrame(mm.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "      ## compute the optimal hyper-parameters on the training set\n",
        "      sampler = optuna.samplers.TPESampler(multivariate=True)\n",
        "      # sampler = optuna.samplers.RandomSampler()\n",
        "      name = model+'_'+str(fold_num)\n",
        "      # default is Tree-structured Parzen Estimator (TPE) optimization algorithm\n",
        "      study = optuna.create_study(direction='maximize', \n",
        "                                  sampler=sampler, \n",
        "                                  study_name=name, \n",
        "                                  pruner=optuna.pruners.HyperbandPruner(min_resource=1, reduction_factor=3))\n",
        "      ## we use X_train in the following, not X_train_scaled, because optuna pipeline in objective function has MinMaxScaler() already\n",
        "      study.optimize(lambda trial: objective(trial, model, X_train, y_train), n_trials=NT)\n",
        "      trial = study.best_trial\n",
        "      best_params = trial.params\n",
        "     \n",
        "      if model == 'logistic_regression': \n",
        "        optimal_clf = LogisticRegression(solver='lbfgs', **best_params)\n",
        "        \n",
        "      elif model == 'linear_svm': \n",
        "        optimal_clf = SVC(kernel='linear', probability=True, **best_params)\n",
        "\n",
        "      elif model == 'rbf_svm': \n",
        "        optimal_clf = SVC(kernel='rbf', probability=True, **best_params)\n",
        "\n",
        "      elif model == 'decision_tree': \n",
        "        feature_names = [f\"feature {i}\" for i in range(X_train.shape[1])]\n",
        "        optimal_clf = DecisionTreeClassifier(**best_params)\n",
        "        \n",
        "\n",
        "      elif model == 'rforest': \n",
        "        optimal_clf = RandomForestClassifier(**best_params)\n",
        "\n",
        "      elif model == 'adaboost':  \n",
        "        optimal_clf = AdaBoostClassifier(**best_params)\n",
        "\n",
        "      elif model == 'xgboost': \n",
        "        optimal_clf = xgb.XGBClassifier(**best_params)\n",
        "\n",
        "      elif model == 'bagging': \n",
        "        optimal_clf = BaggingClassifier(**best_params)\n",
        "        \n",
        "\n",
        "      # now re-train the optimal model on the train set and test on the held-out test set\n",
        "\n",
        "      optimal_clf.fit(X_train_scaled, y_train)\n",
        "      preds = optimal_clf.predict(X_test_scaled)\n",
        "      probas = optimal_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "      acc = accuracy_score(y_test, preds)\n",
        "      try:\n",
        "          auc = roc_auc_score(y_test, probas)\n",
        "      except ValueError:\n",
        "          auc = 0\n",
        "          pass\n",
        "      precision = precision_score(y_test, preds, labels=np.unique(preds))\n",
        "      recall = recall_score(y_test, preds, labels=np.unique(preds))\n",
        "      f1 = f1_score(y_test, preds,  labels=np.unique(preds))\n",
        "      pars = optimal_clf.get_params()\n",
        "      feature_importance = []\n",
        "      # store results \n",
        "      all_model_results.loc[len(all_model_results.index)] = [model, fold_num, acc, auc, precision, recall, f1, feature_importance] \n",
        "      all_values.loc[len(all_values.index)]= [model, fold_num, y_test, probas, preds]\n",
        "      probs.append(probas)\n",
        "      predictions.append(preds)\n",
        "      ground_truths.append(y_test)\n",
        "      model_params[model + '__fold-' + str(fold_num)] = pars\n",
        "\n",
        "      # increment fold_numroc_auc_score\n",
        "      fold_num +=1\n",
        "\n",
        "      # print for sanity\n",
        "      curr_avg_acc = round(all_model_results[all_model_results['model']==model]['accuracy'].mean(), 2) \n",
        "      curr_avg_auc = round(all_model_results[all_model_results['model']==model]['auc'].mean(), 2)\n",
        "      #curr_avg_fi = all_model_results[all_model_results['model']==model]['feature_importance'].mean()\n",
        "      print('participant_test: ' + str(participant_test) +'   acc: ' + str(curr_avg_acc) + '   auc: ' + str(curr_avg_auc))\n",
        "\n",
        "    # store model predictions and ground truths\n",
        "    model_predictions[model] = predictions\n",
        "    model_ground_truths[model] = ground_truths\n",
        "    model_probas[model] = probs\n",
        "    ground_truths_concatenate = np.concatenate( ground_truths, axis=0 )\n",
        "    predictions_concatenate = np.concatenate( predictions, axis=0 )\n",
        "    model_cm[model] = confusion_matrix(ground_truths_concatenate, predictions_concatenate, labels=[0,1])     \n",
        "    #feature_importance = all_model_results[all_model_results['model'] == model]['feature_importance'].mean()   \n",
        "\n",
        "    # verbose\n",
        "    if verbose:\n",
        "      print('model: ', model)\n",
        "      print('mean accuracy: ', all_model_results[all_model_results['model'] == model]['accuracy'].mean())\n",
        "      print('stddev accuracy: ', all_model_results[all_model_results['model'] == model]['accuracy'].std())\n",
        "      print('mean auc: ', all_model_results[all_model_results['model'] == model]['auc'].mean())\n",
        "      print('stddev auc: ', all_model_results[all_model_results['model'] == model]['auc'].std())\n",
        "      print('mean precision: ', all_model_results[all_model_results['model'] == model]['precision'].mean())\n",
        "      print('stddev precision: ', all_model_results[all_model_results['model'] == model]['precision'].std())\n",
        "      print('mean recall: ', all_model_results[all_model_results['model'] == model]['recall'].mean())\n",
        "      print('stddev recall: ', all_model_results[all_model_results['model'] == model]['recall'].std())\n",
        "      print('mean f1: ', all_model_results[all_model_results['model'] == model]['f1'].mean())\n",
        "      print('stddev f1: ', all_model_results[all_model_results['model'] == model]['f1'].std())\n",
        "      print()\n",
        "      print()\n",
        "    print()\n",
        "    \n",
        "  return all_model_results, all_values, model_predictions, model_ground_truths, model_probas, model_features_ranked, model_features_selected, model_params, model_cm, feature_importance"
      ],
      "metadata": {
        "id": "jLKkkOLOZ6lo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_modeling_experiment(df, num_splits, num_repeats, NT, verbose):\n",
        "  model_predictions = {} # predictions made by each model\n",
        "  model_ground_truths = {} # should all be the same, but might as well store these\n",
        "  model_features_ranked = {} # features ranked during features-selection\n",
        "  model_feature_importance = {}\n",
        "  model_features_selected = {} # features selected during feature-selection\n",
        "  model_params = {} # params of trained models\n",
        "  model_probas = {}\n",
        "\n",
        "  # store raw results across all folds\n",
        "  all_model_results = pd.DataFrame(data=None, columns=['model', 'cv_fold', 'accuracy', 'auc', 'precision', 'recall', 'f1'])\n",
        "  all_values = pd.DataFrame(data=None, columns=['model', 'cv_fold', 'ground_truths', 'probas', 'preds'])\n",
        "\n",
        "  df_without_NaN = pre_process_outsideCV(df)\n",
        "  # loop through models\n",
        "  for model in MODELS:\n",
        "    # print model\n",
        "    print(model)\n",
        "\n",
        "    # get X and y\n",
        "    x_old = df_without_NaN\n",
        "    y = df_without_NaN['label']\n",
        "    X = df_without_NaN.drop(NON_MODELING_COLS, axis=1)\n",
        "    ids = x_old[\"filename\"]\n",
        "\n",
        "    # enter cross-validation loop\n",
        "    rskf = RepeatedStratifiedKFold(n_splits=num_splits, n_repeats=num_repeats, random_state = RANDOM_STATE)\n",
        "\n",
        "    # initialize fold numbers and predictions/ground truth lists\n",
        "    fold_num = 0\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    probs = []\n",
        "    features_ranked = []\n",
        "    features_importance = []\n",
        "\n",
        "    for train_index, test_index in rskf.split(X, y):\n",
        "      # get train and test indices\n",
        "\n",
        "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "      y_train, y_test = y.values[train_index], y.values[test_index]\n",
        "      id_test = ids.loc[test_index]\n",
        "      # get the scaled version of the train and test set\n",
        "      mm = MinMaxScaler()\n",
        "      X_train_scaled = pd.DataFrame(mm.fit_transform(X_train), columns= X_train.columns)\n",
        "      X_test_scaled = pd.DataFrame(mm.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "      ## compute the optimal hyper-parameters on the training set\n",
        "      sampler = optuna.samplers.TPESampler(multivariate=True)\n",
        "      # sampler = optuna.samplers.RandomSampler()\n",
        "      name = model+'_'+str(fold_num)\n",
        "      # default is Tree-structured Parzen Estimator (TPE) optimization algorithm\n",
        "      study = optuna.create_study(direction='maximize', \n",
        "                                  sampler=sampler, \n",
        "                                  study_name=name, \n",
        "                                  pruner=optuna.pruners.HyperbandPruner(min_resource=1, reduction_factor=3))\n",
        "      ## we use X_train in the following, not X_train_scaled, because optuna pipeline in objective function has MinMaxScaler() already\n",
        "      study.optimize(lambda trial: objective(trial, model, X_train, y_train), n_trials=NT)\n",
        "      trial = study.best_trial\n",
        "      best_params = trial.params\n",
        "     \n",
        "      if model == 'logistic_regression': \n",
        "        optimal_clf = LogisticRegression(solver='lbfgs', **best_params)\n",
        "        \n",
        "      elif model == 'linear_svm': \n",
        "        optimal_clf = SVC(kernel='linear', probability=True, **best_params)\n",
        "\n",
        "      elif model == 'rbf_svm': \n",
        "        optimal_clf = SVC(kernel='rbf', probability=True, **best_params)\n",
        "\n",
        "      elif model == 'decision_tree': \n",
        "        optimal_clf = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "      elif model == 'rforest': \n",
        "        optimal_clf = RandomForestClassifier(**best_params)\n",
        "\n",
        "      elif model == 'adaboost':  \n",
        "        optimal_clf = AdaBoostClassifier(**best_params)\n",
        "\n",
        "      elif model == 'xgboost': \n",
        "        optimal_clf = xgb.XGBClassifier(**best_params)\n",
        "\n",
        "      elif model == 'bagging': \n",
        "        optimal_clf = BaggingClassifier(**best_params)\n",
        "        \n",
        "\n",
        "      # now re-train the optimal model on the train set and test on the held-out test set\n",
        "\n",
        "      optimal_clf.fit(X_train_scaled, y_train)\n",
        "      preds = optimal_clf.predict(X_test_scaled)\n",
        "      probas = optimal_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "      acc = accuracy_score(y_test, preds)\n",
        "      auc = roc_auc_score(y_test, probas)\n",
        "      precision = precision_score(y_test, preds, labels=np.unique(preds))\n",
        "      recall = recall_score(y_test, preds, labels=np.unique(preds))\n",
        "      f1 = f1_score(y_test, preds,  labels=np.unique(preds))\n",
        "      pars = optimal_clf.get_params()\n",
        "\n",
        "      # store results \n",
        "      all_model_results.loc[len(all_model_results.index)] = [model, fold_num, acc, auc, precision, recall, f1]\n",
        "      all_values.loc[len(all_values.index)]= [mode, model, fold_num, y_test, probas, preds]\n",
        "      probs.append(probas)\n",
        "      predictions.append(preds)\n",
        "      ground_truths.append(y_test)\n",
        "      model_params[model + '__fold-' + str(fold_num)] = pars\n",
        "\n",
        "      # increment fold_num\n",
        "      fold_num +=1\n",
        "\n",
        "      # print for sanity\n",
        "      curr_avg_acc = round(all_model_results[all_model_results['model']==model]['accuracy'].mean(), 2) \n",
        "      curr_avg_auc = round(all_model_results[all_model_results['model']==model]['auc'].mean(), 2)\n",
        "      print('participant_picture_test: ' + str(id_test) +' acc: ' + str(curr_avg_acc) + '   auc: ' + str(curr_avg_auc))\n",
        "\n",
        "    # store model predictions and ground truths\n",
        "    model_predictions[model] = predictions\n",
        "    model_ground_truths[model] = ground_truths\n",
        "    model_probas[model] = probs\n",
        "\n",
        "    # verbose\n",
        "    if verbose:\n",
        "      print('model: ', model)\n",
        "      print('mean accuracy: ', all_model_results[all_model_results['model'] == model]['accuracy'].mean())\n",
        "      print('stddev accuracy: ', all_model_results[all_model_results['model'] == model]['accuracy'].std())\n",
        "      print('mean auc: ', all_model_results[all_model_results['model'] == model]['auc'].mean())\n",
        "      print('stddev auc: ', all_model_results[all_model_results['model'] == model]['auc'].std())\n",
        "      print('mean precision: ', all_model_results[all_model_results['model'] == model]['precision'].mean())\n",
        "      print('stddev precision: ', all_model_results[all_model_results['model'] == model]['precision'].std())\n",
        "      print('mean recall: ', all_model_results[all_model_results['model'] == model]['recall'].mean())\n",
        "      print('stddev recall: ', all_model_results[all_model_results['model'] == model]['recall'].std())\n",
        "      print('mean f1: ', all_model_results[all_model_results['model'] == model]['f1'].mean())\n",
        "      print('stddev f1: ', all_model_results[all_model_results['model'] == model]['f1'].std())\n",
        "      print()\n",
        "      print()\n",
        "    print()\n",
        "  return all_model_results, all_values, model_predictions, model_ground_truths, model_probas, model_features_ranked, model_features_selected, model_params"
      ],
      "metadata": {
        "id": "WC27doGIaCXV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running experiments"
      ],
      "metadata": {
        "id": "eA9Gwq4RtRit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run experiments monomodal for FACE, and AUDIO\n",
        "for mode in MODALITY:\n",
        "  model_results, model_values, model_preds, model_gtruths, model_probas, model_featranks, model_featselected, model_params = run_modeling_experiment(DF_fixed_length[mode], 2, 1, 1, True)\n",
        "  model_results.to_csv(RESULTS_DIR + mode +'_all_model_results.csv')\n",
        "  model_values[\"mode\"] = mode\n",
        "  model_values.to_csv(RESULTS_DIR + mode +'_all_model_values.csv')"
      ],
      "metadata": {
        "id": "dJUieTcZaECv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548a6fd8-62a7-4d65-bfb5-c9514dac446a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  model cv_fold  \\\n",
            "0   logistic_regression       0   \n",
            "1   logistic_regression       1   \n",
            "2               rbf_svm       0   \n",
            "3               rbf_svm       1   \n",
            "4         decision_tree       0   \n",
            "5         decision_tree       1   \n",
            "6            linear_svm       0   \n",
            "7            linear_svm       1   \n",
            "8              adaboost       0   \n",
            "9              adaboost       1   \n",
            "10              xgboost       0   \n",
            "11              xgboost       1   \n",
            "12              bagging       0   \n",
            "13              bagging       1   \n",
            "14              rforest       0   \n",
            "15              rforest       1   \n",
            "\n",
            "                                        ground_truths  \\\n",
            "0   [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "1   [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "2   [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "3   [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "4   [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "5   [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "6   [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "7   [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "8   [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "9   [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "10  [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "11  [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "12  [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "13  [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "14  [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ...   \n",
            "15  [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, ...   \n",
            "\n",
            "                                               probas  \\\n",
            "0   [0.8210297407388228, 0.600502342569875, 0.4985...   \n",
            "1   [0.00017166929774940347, 0.011139409367053264,...   \n",
            "2   [0.5976836647791484, 0.5976836647791484, 0.597...   \n",
            "3   [0.4857730584605545, 0.4857730584605545, 0.485...   \n",
            "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...   \n",
            "5   [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, ...   \n",
            "6   [0.17506616917375403, 0.4641762166772108, 0.95...   \n",
            "7   [0.6743082116774324, 0.6312142791628674, 0.633...   \n",
            "8   [8.025713424340868e-06, 8.025713424340868e-06,...   \n",
            "9   [5.015288091997537e-06, 5.015288091997537e-06,...   \n",
            "10  [0.4365068, 0.33711153, 0.12748726, 0.11452015...   \n",
            "11  [0.24267548, 0.12834874, 0.5140528, 0.91817015...   \n",
            "12  [0.5, 0.5416666666666666, 0.3333333333333333, ...   \n",
            "13  [0.25, 0.375, 0.3333333333333333, 0.7083333333...   \n",
            "14  [0.6666666666666666, 0.7777777777777778, 0.777...   \n",
            "15  [0.2530831477890302, 0.6754677185559538, 0.567...   \n",
            "\n",
            "                                                preds  mode  \n",
            "0   [1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, ...  face  \n",
            "1   [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, ...  face  \n",
            "2   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  face  \n",
            "3   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  face  \n",
            "4   [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, ...  face  \n",
            "5   [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, ...  face  \n",
            "6   [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, ...  face  \n",
            "7   [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, ...  face  \n",
            "8   [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, ...  face  \n",
            "9   [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, ...  face  \n",
            "10  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, ...  face  \n",
            "11  [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  face  \n",
            "12  [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, ...  face  \n",
            "13  [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, ...  face  \n",
            "14  [1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, ...  face  \n",
            "15  [0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, ...  face  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# leave one week and subject out\n",
        "for mode in MODALITY:\n",
        "  group = \"WEEK\"\n",
        "  model_results, model_values, model_preds, model_gtruths, model_probas, model_featranks, model_featselected, model_params = run_modeling_experiment_logo(DF_fixed_length[mode], NUM_SPLITS, NUM_REPEATS, 50, True, group)\n",
        "  model_results.to_csv(RESULTS_DIR + mode +'_'+group+'_all_model_results.csv')\n",
        "  group = \"SUBJECT\"\n",
        "  model_results, model_values, model_preds, model_gtruths, model_probas, model_featranks, model_featselected, model_params = run_modeling_experiment_logo(DF_fixed_length[mode], NUM_SPLITS, NUM_REPEATS, 50, True, group)\n",
        "  model_results.to_csv(RESULTS_DIR + mode +'_'+group+'_all_model_results.csv')"
      ],
      "metadata": {
        "id": "c88mzQB8A9b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multimodal modeling**"
      ],
      "metadata": {
        "id": "9qs7nNa7qZMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Early Fusion**\n",
        "Fuse the audio, face, and text data at the feature-level. "
      ],
      "metadata": {
        "id": "ozmP_NcpqepJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concatenate feature vectors"
      ],
      "metadata": {
        "id": "EjpZZoMBzmpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the label proportions\n",
        "DF_fixed_length = {}\n",
        "for mode in MODALITY:\n",
        "  DF_fixed_length[mode] = pd.read_csv(FEATURE_DIR + mode+'_fixed_length_vectors.csv')\n",
        "  label_proportion = DF_fixed_length[mode]['label'].sum()/len(DF_fixed_length[mode])\n",
        "  print('BASELINE (classifier that always predicts \"Positive Affect level\"): ', label_proportion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3cCJslRyf__",
        "outputId": "472470d9-e5e4-4666-b561-a6bf82f9d348"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE (classifier that always predicts \"Positive Affect level\"):  0.5853658536585366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([DF_fixed_length[MODALITY[0]], DF_fixed_length[MODALITY[1]]],  axis=1, join=\"inner\")\n",
        "df_early_fusion = pre_process_outsideCV(df) # df without NaN values\n",
        "print(df_early_fusion.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "RhVTputy0Gwb",
        "outputId": "f1c69467-1d2c-4f74-f55c-4f8593dd28f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2831eb512de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDF_fixed_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMODALITY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDF_fixed_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMODALITY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_early_fusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_outsideCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# df without NaN values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_early_fusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running experiments"
      ],
      "metadata": {
        "id": "y4fQp6hj3Xut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run experiments monomodal for concatenated feature vector\n",
        "model_results, model_values, model_preds, model_gtruths, model_probas, model_featranks, model_featselected, model_params = run_modeling_experiment(df_early_fusion, NUM_SPLITS, NUM_REPEATS, 50, True)\n",
        "model_results.to_csv(RESULTS_DIR + mode +'_all_model_results_early_fusion.csv')"
      ],
      "metadata": {
        "id": "ElwCaPcQ3ZpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# leave one week and subject out\n",
        "group = \"WEEK\"\n",
        "model_results, model_values, model_preds, model_gtruths, model_probas, model_featranks, model_featselected, model_params = run_modeling_experiment_logo(df_early_fusion, NUM_SPLITS, NUM_REPEATS, 50, True, group)\n",
        "model_results.to_csv(RESULTS_DIR + mode +'_'+group+'_all_model_results_early_fusion.csv')\n",
        "group = \"SUBJECT\"\n",
        "model_results, model_values, model_preds, model_gtruths, model_probas, model_featranks, model_featselected, model_params = run_modeling_experiment_logo(df_early_fusion, NUM_SPLITS, NUM_REPEATS, 50, True, group)\n",
        "model_results.to_csv(RESULTS_DIR + mode +'_'+group+'_all_model_results_early_fusion.csv')"
      ],
      "metadata": {
        "id": "cHvgZWQdBFL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Late Fusion**\n",
        "Concatenation of modalities at decision-level. Fusion mechanisms:  averaging, voting schemes, weighting based on channel noise and signal variance, or a learned model (see [this paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8269806&fbclid=IwAR1C_TKJXvLIdOsgkOiwX_A10pZocHEiBOvhgwjeYgWiTf9B7_N3PiszMQM&tag=1)) "
      ],
      "metadata": {
        "id": "Pwpzqs5Lqhs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataframe creation from unimodal predictions"
      ],
      "metadata": {
        "id": "Ig_2__dGJM70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_metrics(gt, preds, probas):\n",
        "  acc = accuracy_score(gt, preds)\n",
        "  auc = roc_auc_score(gt, probas)\n",
        "  precision = precision_score(gt, preds, labels=np.unique(preds))\n",
        "  recall = recall_score(gt, preds, labels=np.unique(preds))\n",
        "  f1 = f1_score(gt, preds,  labels=np.unique(preds))\n",
        "  return acc, auc, precision, recall, f1"
      ],
      "metadata": {
        "id": "YWNcmWtARpKu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "df = {}\n",
        "MODALITY = [\"audio\", \"face\"]\n",
        "for mode in MODALITY:\n",
        "  df[mode] = pd.read_csv(RESULTS_DIR + mode +'_all_model_values.csv')\n",
        "df = pd.concat([df[MODALITY[0]],df[MODALITY[1]]], ignore_index = True)#, df[MODALITY[1]]])"
      ],
      "metadata": {
        "id": "T9ya60KqJKFr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Major voting (hard and soft)\n",
        "The final classifier decision is either the class label predicted most frequently by the unimodal classifiers (hard majority voting) or the class label with the highest average predicted class probability across the unimodal classifiers (soft majority voting)."
      ],
      "metadata": {
        "id": "tTOm9LinFFzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def major_voting_late_fusion(df, mv_type):\n",
        "  #mv_type (string): major voting type: hard or soft\n",
        "  # store raw results across all folds\n",
        "  all_model_results = pd.DataFrame(data=None, columns=['model', 'cv_fold', 'accuracy', 'auc', 'precision', 'recall', 'f1'])\n",
        "  for model in MODELS:\n",
        "    df_model = df.loc[df['model'] == model]\n",
        "    # define as many df_model as the number of modalities to take into account\n",
        "    df_model_mode1 = df_model.loc[df_model['mode'] == MODALITY[0]].reset_index() \n",
        "    df_model_mode2 = df_model.loc[df_model['mode'] == MODALITY[1]].reset_index() \n",
        "    #loop through folds:\n",
        "    for index, row in df_model_mode1.iterrows():\n",
        "      fold_num = df_model_mode1[\"cv_fold\"][index]\n",
        "      gt = [float(i) for i in ((df_model_mode1[\"ground_truths\"][index].replace(\"[\",\"\")).replace(\"]\", \"\")).split()]\n",
        "      pred_1 = [float(i) for i in ((df_model_mode1[\"preds\"][index].replace(\"[\",\"\")).replace(\"]\", \"\")).split()] \n",
        "      pred_2 =  [float(i) for i in((df_model_mode2[\"preds\"][index].replace(\"[\",\"\")).replace(\"]\", \"\")).split()]\n",
        "      prob_1 =  [float(i) for i in ((df_model_mode1[\"probas\"][index].replace(\"[\",\"\")).replace(\"]\", \"\")).split()]\n",
        "      prob_2 = [float(i) for i in((df_model_mode2[\"probas\"][index].replace(\"[\",\"\")).replace(\"]\", \"\")).split()]\n",
        "      pred_major_voting = []\n",
        "      prob_major_voting = []\n",
        "      for i in range(0,len(pred_1)):\n",
        "        if mv_type == \"hard\":\n",
        "          if pred_1[i] >= pred_2[i]:\n",
        "            pred_major_voting.append(pred_1[i])\n",
        "            prob_major_voting.append(prob_1[i])\n",
        "          elif pred_1[i] < pred_2[i]:\n",
        "            pred_major_voting.append(pred_2[i])\n",
        "            prob_major_voting.append(prob_2[i])\n",
        "        elif mv_type == \"soft\":\n",
        "          if prob_1[i] >= prob_2[i]:\n",
        "            prob_major_voting.append(prob_1[i])\n",
        "            pred_major_voting.append(pred_1[i])\n",
        "          elif prob_1[i] < prob_2[i]:\n",
        "            prob_major_voting.append(prob_2[i])\n",
        "            pred_major_voting.append(pred_2[i])\n",
        "      [acc, auc, precision, recall, f1] =  evaluation_metrics(gt, pred_major_voting, prob_major_voting)\n",
        "      all_model_results.loc[len(all_model_results.index)] = [model, fold_num, acc, auc, precision, recall, f1]\n",
        "      print(all_model_results.head())\n",
        "  return all_model_results\n"
      ],
      "metadata": {
        "id": "XhAKRafaNsju"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## for each model, get the most frequent class label predicted for each unimodal classifiers \n",
        "MV_TYPES = [\"hard\", \"soft\"]\n",
        "for mv_type in MV_TYPES:\n",
        "  model_results = major_voting_late_fusion(df, mv_type)\n",
        "  model_results.to_csv(RESULTS_DIR + 'multimodal_all_model_results_major_voting_'+mv_type+'.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSgCoeMg8phv",
        "outputId": "4835eaf9-74c7-4f11-9a4f-302f1f31a0c5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.714286  0.675926   0.714286  0.833333   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.769231  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision  recall    f1\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308    0.75  0.72\n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n",
            "                 model cv_fold  accuracy       auc  precision    recall  \\\n",
            "0  logistic_regression       0  0.666667  0.675926   0.692308  0.750000   \n",
            "1  logistic_regression       1  0.700000  0.729167   0.800000  0.666667   \n",
            "2              rbf_svm       0  0.571429  0.500000   0.571429  1.000000   \n",
            "3              rbf_svm       1  0.600000  0.500000   0.600000  1.000000   \n",
            "4        decision_tree       0  0.476190  0.486111   0.555556  0.416667   \n",
            "\n",
            "         f1  \n",
            "0  0.720000  \n",
            "1  0.727273  \n",
            "2  0.727273  \n",
            "3  0.750000  \n",
            "4  0.476190  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking (hard and soft)\n",
        "Building a meta-model (it gives more context). A final classifier is trained on either the predicted class labels of the unimodal classifiers (hard stacking) or the predicted class probabilities of the unimodal classifiers (soft stacking)."
      ],
      "metadata": {
        "id": "Dz4jystJFLxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NON_MODELING_COLS = ['model', 'cv_fold', 'mode']"
      ],
      "metadata": {
        "id": "QAbFYzzpHpl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "# TODOO!!\n",
        "def stacking(df, num_splits, num_repeats, NT, verbose):\n",
        "  all_model_results = pd.DataFrame(data=None, columns=['model', 'cv_fold', 'accuracy', 'auc', 'precision', 'recall', 'f1'])\n",
        "\n",
        "  df_without_NaN = pre_process_outsideCV(df)\n",
        "  # loop through models\n",
        "  for model in MODELS:\n",
        "    # print model\n",
        "    print(model)\n",
        "\n",
        "    # get X and y\n",
        "    x_old = df_without_NaN\n",
        "    y = df_without_NaN['label']\n",
        "    X = df_without_NaN.drop(NON_MODELING_COLS, axis=1)\n",
        "    ids = x_old[\"filename\"]\n",
        "\n",
        "    # enter cross-validation loop\n",
        "    rskf = RepeatedStratifiedKFold(n_splits=num_splits, n_repeats=num_repeats, random_state = RANDOM_STATE)\n",
        "\n",
        "    # initialize fold numbers and predictions/ground truth lists\n",
        "    fold_num = 0\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    probs = []\n",
        "    features_ranked = []\n",
        "    features_importance = []\n",
        "\n",
        "    for train_index, test_index in rskf.split(X, y):\n",
        "      # get train and test indices\n",
        "\n",
        "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "      y_train, y_test = y.values[train_index], y.values[test_index]\n",
        "      id_test = ids.loc[test_index]\n",
        "      # get the scaled version of the train and test set\n",
        "      mm = MinMaxScaler()\n",
        "      X_train_scaled = pd.DataFrame(mm.fit_transform(X_train), columns= X_train.columns)\n",
        "      X_test_scaled = pd.DataFrame(mm.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "      ## compute the optimal hyper-parameters on the training set\n",
        "      sampler = optuna.samplers.TPESampler(multivariate=True)\n",
        "      # sampler = optuna.samplers.RandomSampler()\n",
        "      name = model+'_'+str(fold_num)\n",
        "      # default is Tree-structured Parzen Estimator (TPE) optimization algorithm\n",
        "      study = optuna.create_study(direction='maximize', \n",
        "                                  sampler=sampler, \n",
        "                                  study_name=name, \n",
        "                                  pruner=optuna.pruners.HyperbandPruner(min_resource=1, reduction_factor=3))\n",
        "      ## we use X_train in the following, not X_train_scaled, because optuna pipeline in objective function has MinMaxScaler() already\n",
        "      study.optimize(lambda trial: objective(trial, model, X_train, y_train), n_trials=NT)\n",
        "      trial = study.best_trial\n",
        "      best_params = trial.params\n",
        "     \n",
        "      if model == 'logistic_regression': \n",
        "        optimal_clf = LogisticRegression(solver='lbfgs', **best_params)\n",
        "        \n",
        "      elif model == 'linear_svm': \n",
        "        optimal_clf = SVC(kernel='linear', probability=True, **best_params)\n",
        "\n",
        "      elif model == 'rbf_svm': \n",
        "        optimal_clf = SVC(kernel='rbf', probability=True, **best_params)\n",
        "\n",
        "      elif model == 'decision_tree': \n",
        "        optimal_clf = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "      elif model == 'rforest': \n",
        "        optimal_clf = RandomForestClassifier(**best_params)\n",
        "\n",
        "      elif model == 'adaboost':  \n",
        "        optimal_clf = AdaBoostClassifier(**best_params)\n",
        "\n",
        "      elif model == 'xgboost': \n",
        "        optimal_clf = xgb.XGBClassifier(**best_params)\n",
        "\n",
        "      elif model == 'bagging': \n",
        "        optimal_clf = BaggingClassifier(**best_params)\n",
        "        \n",
        "\n",
        "      # now re-train the optimal model on the train set and test on the held-out test set\n",
        "      estimators = [optimal_clf_1, optimal_clf_2]\n",
        "      stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(solver='lbfgs', **best_params))\n",
        "      stacking_clf.fit(X_train_scaled, y_train)\n",
        "      preds = stacking_clf.predict(X_test_scaled)\n",
        "      probas = stacking_clf.predict_proba(X_test_scaled)[:, 1]\n",
        "      acc = accuracy_score(y_test, preds)\n",
        "      auc = roc_auc_score(y_test, probas)\n",
        "      precision = precision_score(y_test, preds, labels=np.unique(preds))\n",
        "      recall = recall_score(y_test, preds, labels=np.unique(preds))\n",
        "      f1 = f1_score(y_test, preds,  labels=np.unique(preds))\n",
        "      pars = stacking_clf.get_params()\n",
        "\n",
        "      # store results \n",
        "      all_model_results.loc[len(all_model_results.index)] = [model, fold_num, acc, auc, precision, recall, f1]\n",
        "      all_values.loc[len(all_values.index)]= [mode, model, fold_num, y_test, probas, preds]\n",
        "      probs.append(probas)\n",
        "      predictions.append(preds)\n",
        "      ground_truths.append(y_test)\n",
        "      model_params[model + '__fold-' + str(fold_num)] = pars\n",
        "\n",
        "      # increment fold_num\n",
        "      fold_num +=1\n",
        "\n",
        "      # print for sanity\n",
        "      curr_avg_acc = round(all_model_results[all_model_results['model']==model]['accuracy'].mean(), 2) \n",
        "      curr_avg_auc = round(all_model_results[all_model_results['model']==model]['auc'].mean(), 2)\n",
        "      print('participant_picture_test: ' + str(id_test) +' acc: ' + str(curr_avg_acc) + '   auc: ' + str(curr_avg_auc))\n",
        "\n",
        "    # store model predictions and ground truths\n",
        "    model_predictions[model] = predictions\n",
        "    model_ground_truths[model] = ground_truths\n",
        "    model_probas[model] = probs\n",
        "\n",
        "    # verbose\n",
        "    if verbose:\n",
        "      print('model: ', model)\n",
        "      print('mean accuracy: ', all_model_results[all_model_results['model'] == model]['accuracy'].mean())\n",
        "      print('stddev accuracy: ', all_model_results[all_model_results['model'] == model]['accuracy'].std())\n",
        "      print('mean auc: ', all_model_results[all_model_results['model'] == model]['auc'].mean())\n",
        "      print('stddev auc: ', all_model_results[all_model_results['model'] == model]['auc'].std())\n",
        "      print('mean precision: ', all_model_results[all_model_results['model'] == model]['precision'].mean())\n",
        "      print('stddev precision: ', all_model_results[all_model_results['model'] == model]['precision'].std())\n",
        "      print('mean recall: ', all_model_results[all_model_results['model'] == model]['recall'].mean())\n",
        "      print('stddev recall: ', all_model_results[all_model_results['model'] == model]['recall'].std())\n",
        "      print('mean f1: ', all_model_results[all_model_results['model'] == model]['f1'].mean())\n",
        "      print('stddev f1: ', all_model_results[all_model_results['model'] == model]['f1'].std())\n",
        "      print()\n",
        "      print()\n",
        "    print()\n",
        "  return all_model_results, all_values, model_predictions, model_ground_truths, model_probas, model_features_ranked, model_features_selected, model_params\n",
        "  "
      ],
      "metadata": {
        "id": "M8zify5kFFYo"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid Stacking (hard and soft)\n",
        "An early fusion vector with features from a set of modalities is concatenated with either the predicted class labels of the set of corresponding unimodal classifiers (hard hybrid fusion) or the predicted class probabilities of the set of corresponding unimodal classifiers (soft hybrid fusion), in order to create final feature vectors that are used in a classifier."
      ],
      "metadata": {
        "id": "Fy8__3wLGjBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Others approaches (not implemented)\n",
        "**Bagging** \n",
        "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n",
        "sklearn.ensemble.BaggingClassifier\n"
      ],
      "metadata": {
        "id": "tnS1WRh1GvYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting**\n",
        "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
      ],
      "metadata": {
        "id": "abv8aKaDGxnj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MukuO_u-HYwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}